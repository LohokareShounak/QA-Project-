{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tQLFfLei1uQ6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (4.8.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (4.61.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (2021.7.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: packaging in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\shounak lohokare\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForQuestionAnswering,\n",
    "    BertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    DistilBertTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    squad_convert_examples_to_features,\n",
    ")\n",
    "\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_log_probs,\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor, squad_convert_example_to_features\n",
    "from transformers import (\n",
    "    BertTokenizer\n",
    ")\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import timeit\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "  \n",
    "def load_and_cache_examples(tokenizer,evaluate = False, output_examples = False):\n",
    "  local_rank = -1\n",
    "  data_dir = None\n",
    "  model_name_or_path = \"bert-base-cased\"\n",
    "  max_seq_length = 384\n",
    "  overwrite_cache = False\n",
    "  predict_file = None\n",
    "  train_file = None\n",
    "  version_2_with_negative = False\n",
    "  doc_stride = 128\n",
    "  max_query_length = 64\n",
    "  threads = 1\n",
    "\n",
    "  if local_rank not in [-1, 0] and not evaluate:\n",
    "        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    # Load data features from cache or dataset file\n",
    "  input_dir = data_dir if data_dir else \".\"\n",
    "  cached_features_file = os.path.join(\n",
    "      input_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            \"dev\" if evaluate else \"train\",\n",
    "            list(filter(None, model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(max_seq_length),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Init features and dataset from cache if it exists\n",
    "  if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "        features_and_dataset = torch.load(cached_features_file)\n",
    "        features, dataset, examples = (\n",
    "            features_and_dataset[\"features\"],\n",
    "            features_and_dataset[\"dataset\"],\n",
    "            features_and_dataset[\"examples\"],\n",
    "        )\n",
    "  else:\n",
    "\n",
    "        if not data_dir and ((evaluate and not predict_file) or (not evaluate and not train_file)):\n",
    "            try:\n",
    "                import tensorflow_datasets as tfds\n",
    "            except ImportError:\n",
    "                raise ImportError(\"If not data_dir is specified, tensorflow_datasets needs to be installed.\")\n",
    "\n",
    "            if version_2_with_negative:\n",
    "                logger.warn(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n",
    "\n",
    "            tfds_examples = tfds.load(\"squad\")\n",
    "            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n",
    "        else:\n",
    "            processor = SquadV2Processor() if version_2_with_negative else SquadV1Processor()\n",
    "            if evaluate:\n",
    "                examples = processor.get_dev_examples(data_dir, filename=predict_file)\n",
    "            else:\n",
    "                examples = processor.get_train_examples(data_dir, filename=train_file)\n",
    "        gc.collect()\n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "            examples=examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=not evaluate,\n",
    "            return_dataset=\"pt\",\n",
    "            threads=threads,\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        if local_rank in [-1, 0]:\n",
    "            \n",
    "            torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
    "\n",
    "  if local_rank == 0 and not evaluate:\n",
    "      # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "      torch.distributed.barrier()\n",
    "\n",
    "  if output_examples:\n",
    "        return dataset, examples, features\n",
    "  return dataset\n",
    "\n",
    "def Train(train_dataset,model,tokenizer):\n",
    "  train_batch_size = 8\n",
    "  weight_decay = 0.0\n",
    "  learning_rate = 5e-5\n",
    "  adam_epsilon = 1e-8\n",
    "  warmup_steps = 0\n",
    "  gradient_accumulation_steps = 1\n",
    "  num_train_epochs = 3.0 \n",
    "  per_gpu_train_batch_size = 8\n",
    "  local_rank = -1\n",
    "  max_grad_norm = 1.0\n",
    "  max_steps = -1\n",
    "  logging_step = 500\n",
    "  save_steps = 500\n",
    "  evaluate_during_training = False\n",
    "  \n",
    "  if local_rank in [-1, 0]:\n",
    "    tb_writer = SummaryWriter()\n",
    "  train_sample = RandomSampler(train_dataset)\n",
    "  train_dataloader = DataLoader(train_dataset,sampler = train_sample, batch_size= train_batch_size)\n",
    "\n",
    "  t_total = len(train_dataloader)//gradient_accumulation_steps*num_train_epochs\n",
    "  no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "  optimizer_grouped_parameters = [\n",
    "      {\n",
    "          \"params\" : [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "          \"weight_decay\" : weight_decay\n",
    "      },\n",
    "      {\n",
    "          \"params\" : [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0 \n",
    "      },\n",
    "  ]\n",
    "\n",
    "  optimizer = AdamW(optimizer_grouped_parameters,lr = learning_rate,eps = adam_epsilon)\n",
    "  scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer, num_warmup_steps = warmup_steps, num_training_steps = t_total\n",
    "  )\n",
    "\n",
    "  global_step = 1\n",
    "  epochs_trained = 0\n",
    "  steps_trained_in_current_epoch = 0\n",
    "\n",
    "  tr_loss , logging_loss = 0.0, 0.0\n",
    "  model.zero_grad()\n",
    "\n",
    "  train_iterater = trange(\n",
    "      epochs_trained, int(num_train_epochs),desc =\"Epoch\",disable = local_rank not in [-1,0]\n",
    "  )\n",
    "\n",
    "\n",
    "  for _ in train_iterater:\n",
    "    epoch_iterator = tqdm(train_dataloader,desc = \"Iteration\",disable = local_rank not in [-1.0])\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "      #Skip past any already trained steps if resuming training\n",
    "      if steps_trained_in_current_epoch > 0:\n",
    "        steps_trained_in_current_epoch -= 1\n",
    "        continue\n",
    "      device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "      model.train()\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "      inputs = {\n",
    "          \"input_ids\" : batch[0],\n",
    "          \"attention_mask\" : batch[1],\n",
    "          \"token_type_ids\" : batch[2],\n",
    "          \"start_positions\" : batch[3],\n",
    "          \"end_positions\" : batch[4],\n",
    "      }\n",
    "\n",
    "      outputs = model(**inputs)\n",
    "\n",
    "      loss = outputs[0]\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      tr_loss += loss.item()\n",
    "\n",
    "      if (step + 1)% gradient_accumulation_steps == 0:\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if local_rank in [-1,0] and logging_step > 0 and global_step % logging_step == 0:\n",
    "\n",
    "          if local_rank == -1 and evaluate_during_training:\n",
    "            results = evaluate(model,tokenizer)\n",
    "\n",
    "            for key, value in results.items():\n",
    "              tb_writer.add_scalar(\"eval_{}\".format(key),value,global_step)\n",
    "            \n",
    "            tb_writer.add_scalar(\"lr\",scheduler.get_lr()[0],global_step)\n",
    "            tb_writer.add_scaler(\"loss\", (tr_loss - logging_loss) / logging_step,global_step)\n",
    "            logging_loss = tr_loss\n",
    "        if local_rank in [-1,0] and save_steps > 0 and global_step % save_steps == 0:\n",
    "          output_dir = os.path.join(\"/Output\",\"checkpoint- {}\".format(global_step))\n",
    "          if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "          \n",
    "          # Take care of distributed/parallel training\n",
    "          model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "          model_to_save.save_pretrained(output_dir)\n",
    "          tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "          torch.save(os.path.join(output_dir, \"training_args.bin\"))\n",
    "          \n",
    "\n",
    "          torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "          torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "      \n",
    "      \n",
    "      if max_steps > 0 and global_step > max_steps:\n",
    "        epoch_iterator.close()\n",
    "        break\n",
    "    if max_steps > 0 and global_step > max_steps:\n",
    "      train_iterater.close()\n",
    "      break\n",
    "  if local_rank in [-1, 0]:\n",
    "    tb_writer.close()\n",
    "\n",
    "  return global_step, tr_loss / global_step\n",
    "\n",
    "def evaluate(model, tokenizer,prefix = \"\"):\n",
    "  output_dir = None\n",
    "  per_gpu_eval_batch_size = 8\n",
    "  n_gpu = 1\n",
    "  n_best_size = 20\n",
    "  max_answer_length = 30\n",
    "  do_lower_case = True\n",
    "  verbose_logging = True\n",
    "  version_2_with_negative = False\n",
    "  null_score_diff_threshold = 0.0\n",
    "  local_rank = -1\n",
    "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "  dataset,examples, features = load_and_cache_examples(tokenizer, evaluate = True, output_examples = True)\n",
    "\n",
    "  if not os.path.exists(output_dir) and local_rank in [-1,0]:\n",
    "    os.makedirs(output_dir)\n",
    "  eval_batch_size = per_gpu_eval_batch_size\n",
    "  \n",
    "  eval_sampler = SequentialSampler(dataset)\n",
    "  eval_dataloader = DataLoader(dataset,sampler = eval_sampler, batch_size = eval_batch_size)\n",
    "\n",
    "\n",
    "  all_results = []\n",
    "  start_time = timeit.default_timer()\n",
    "\n",
    "  for batch in tqdm(eval_dataloader, desc = \"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      input = {\n",
    "          \"input_id\" : batch[0],\n",
    "          \"attention_mask\" : batch[1],\n",
    "          \"token_type_ids\" : batch[2],\n",
    "      }\n",
    "\n",
    "      example_indices = batch[3]\n",
    "\n",
    "      outputs = model(**input)\n",
    "    \n",
    "    for i , example_index in enumerate(example_indices):\n",
    "      eval_feature = features[example_index.item()]\n",
    "      unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "      output = [to_list(output) for output in outputs]\n",
    "\n",
    "      if len(output) >= 5:\n",
    "        start_logits = output[0]\n",
    "        start_top_index = output[1]\n",
    "        end_logits = output[2]\n",
    "        end_top_index = output[3]\n",
    "        cls_logits = output[4]\n",
    "\n",
    "        result = SquadResult(\n",
    "            unique_id,\n",
    "            start_logits,\n",
    "            end_logits,\n",
    "            start_top_index=start_top_index,\n",
    "            end_top_index=end_top_index,\n",
    "            cls_logits=cls_logits,\n",
    "        )\n",
    "      else:\n",
    "        start_logits, end_logits = output\n",
    "        result = SquadResult(unique_id, start_logits, end_logits)\n",
    "      \n",
    "      all_results.append(result)\n",
    "\n",
    "  evalTime = timeit.default_timer() - start_time\n",
    "  \n",
    "\n",
    "  # Compute predictions\n",
    "  output_prediction_file = os.path.join(output_dir, \"predictions_{}.json\".format(prefix))\n",
    "  output_nbest_file = os.path.join(output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "\n",
    "  output_null_log_odds_file = None\n",
    "\n",
    "  predictions = compute_predictions_logits(\n",
    "            examples,\n",
    "            features,\n",
    "            all_results,\n",
    "            n_best_size,\n",
    "            max_answer_length,\n",
    "            do_lower_case,\n",
    "            output_prediction_file,\n",
    "            output_nbest_file,\n",
    "            output_null_log_odds_file,\n",
    "            verbose_logging,\n",
    "            version_2_with_negative,\n",
    "            null_score_diff_threshold,\n",
    "            tokenizer,\n",
    "  )\n",
    "\n",
    "  results = squad_evaluate(examples, predictions)\n",
    "\n",
    "  return results\n",
    "\n",
    "def Save_Model(model,tokenizer):\n",
    "  do_train = True\n",
    "  local_rank = -1\n",
    "  output_dir = \"/Output\"\n",
    "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "  do_lower_case = True\n",
    "  if do_train and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "          # Create output directory if needed\n",
    "          if not os.path.exists(output_dir) and local_rank in [-1, 0]:\n",
    "              os.makedirs(output_dir)\n",
    "\n",
    "    \n",
    "          # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "          # They can then be reloaded using `from_pretrained()`\n",
    "          # Take care of distributed/parallel training\n",
    "          model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "          model_to_save.save_pretrained(output_dir)\n",
    "          tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "          # Good practice: save your training arguments together with the trained model\n",
    "          torch.save(os.path.join(output_dir, \"training_args.bin\"))\n",
    "\n",
    "          # Load a trained model and vocabulary that you have fine-tuned\n",
    "          model = BertForQuestionAnswering.from_pretrained(output_dir)  # , force_download=True)\n",
    "          tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=do_lower_case)\n",
    "          model.to(device)\n",
    "\n",
    "def Evaluate_loop():\n",
    "  do_eval = True\n",
    "  local_rank = -1\n",
    "  results = {}\n",
    "  do_train = True\n",
    "  output_dir = \"/Output\"\n",
    "  eval_all_checkpoints = True\n",
    "  model_name_or_path = \"bert-base-cased\"\n",
    "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "  if do_eval and local_rank in [-1, 0]:\n",
    "      if do_train:\n",
    "          checkpoints = [output_dir]\n",
    "          if eval_all_checkpoints:\n",
    "              checkpoints = list(\n",
    "                  os.path.dirname(c)\n",
    "                  for c in sorted(glob.glob(output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "              )\n",
    "              logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
    "      else:\n",
    "          logger.info(\"Loading checkpoint %s for evaluation\", model_name_or_path)\n",
    "          checkpoints = [model_name_or_path]\n",
    "\n",
    "      for checkpoint in checkpoints:\n",
    "          # Reload the model\n",
    "          global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "          model = BertForQuestionAnswering.from_pretrained(checkpoint)  # , force_download=True)\n",
    "          model.to(device)\n",
    "\n",
    "          # Evaluate\n",
    "          result = evaluate(model, tokenizer, prefix=global_step)\n",
    "\n",
    "          result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
    "          results.update(result)\n",
    "  return results\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACJSn9kS74Y0"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "  tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "  config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "  model = BertForQuestionAnswering.from_pretrained(\"bert-base-cased\",config = config)\n",
    "  model.to(device)\n",
    "\n",
    "  logger = logging.getLogger(__name__)\n",
    "  train_dataset = load_and_cache_examples(tokenizer, evaluate=False, output_examples=False)\n",
    "\n",
    "  global_step, tr_loss = Train(train_dataset, model, tokenizer)\n",
    "  logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "  Save_Model(model, tokenizer)\n",
    "\n",
    "  Results = Evaluate_loop(model)\n",
    "\n",
    "  print(Results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bert_QA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
